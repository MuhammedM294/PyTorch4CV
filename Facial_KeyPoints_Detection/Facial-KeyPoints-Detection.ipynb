{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef3b417",
   "metadata": {},
   "source": [
    "#### Building CNN to Detect Facial Key Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbf97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchsummary import summary\n",
    "import numpy as np, pandas as pd, os, glob, cv2\n",
    "from torch.utils.data import TensorDataset,DataLoader,Dataset\n",
    "from copy import deepcopy\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aee66b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import cluster\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c72258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/udacity/P1_Facial_Keypoints.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "039a390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data/training_frames_keypoints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df4679d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceData(Dataset):\n",
    "    def __init__(self,df):\n",
    "        super(FaceData).__init__()\n",
    "        self.df = df\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,indx):\n",
    "        img_path = 'data/data/training/' + self.df.iloc[indx,0]\n",
    "        img = cv2.imread(img_path)/255\n",
    "        keypoints = deepcopy(self.pd.iloc[indx,1:].tolist())\n",
    "        keypoints_x = (np.array(keypoints[0::2])/img.shape[1]).tolist()\n",
    "        keypoints_y = (np.array(keypoints[0::2])/img.shape[1]).tolist()\n",
    "        keypoints_2 = keypoints_x+keypoints_y\n",
    "        keypoints_2 = torch.tensor(keypoints_2)\n",
    "        img = self.preprocess_input(img)\n",
    "        \n",
    "        return img, keypoints_2\n",
    "    def preprocess_input(self,img):\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "        img = self.normalize(img).float()\n",
    "        \n",
    "        return img.to(device)\n",
    "    def load_image(self,indx):\n",
    "        img_path = 'data/data/training/' + self.df.iloc[indx,0]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        \n",
    "        return img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34d19ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data  = train_test_split(data,test_size=0.2,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d25a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FaceData(train_data.reset_index(drop=True))\n",
    "test_dataset = FaceData(test_data.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3956bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34924415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.avgpool = nn.Sequential( nn.Conv2d(512,512,3),\n",
    "                                  nn.MaxPool2d(2),\n",
    "                                  nn.Flatten())\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(2048, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 136),\n",
    "        nn.Sigmoid())\n",
    "    loss_function = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    return model.to(device), loss_function, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db491fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m294/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/m294/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model, loss_function,optimizer = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2695365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
