# -*- coding: utf-8 -*-
"""pytorch09.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12oG0xUV9IbeXgV93H_C9zZ9s0kTiEO63

## **1. Import the neccessary packages**
"""

# Commented out IPython magic to ensure Python compatibility.
import torch 
import torchvision 
import torch.nn as nn 
import torch.nn.functional as F
import matplotlib.pyplot as plt
import cv2 , glob, numpy as np , pandas as pd 
from torchvision import datasets , transforms, models
from torch.utils.data import Dataset, DataLoader
from torch import optim 
from PIL import Image 
from random import shuffle, seed; seed(10);
from glob import glob 
# %matplotlib inline

"""# 2. Download the Dataset

## 2.1 Upload Kaggle authentication file
"""

!pip install -q kaggle 
from google.colab import files

files.upload()

"""## 2.2 Moving to Kaggle folder and copy the Kaggle.json file to it."""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json

"""## 2.3 Downloading the cats/dogs Dataset and unzip it"""

!kaggle datasets download -d tongpython/cat-and-dog
!unzip cat-and-dog.zip

"""# 3. Provide the training and test datasets"""

train_data = '/content/training_set/training_set'
test_data = '/content/test_set/test_set'

"""# 4. Build a class that fetch the data from the preceding folders"""

class Cat_Dog(Dataset):

  def __init__(self,folder):
    cats = glob(folder+'/cats/*.jpg')
    dogs = glob(folder+'/dogs/*.jpg')
    self.fpaths = cats + dogs

    shuffle(self.fpaths)
    self.targets = [fpath.split('/')[-1].startswith('dog') for fpath in self.fpaths] # dog = 1
  
  def __len__(self):
    return len(self.fpaths) 

  def __getitem__(self, ix):
    f = self.fpaths[ix]
    target = self.targets[ix]
    img = (cv2.imread(f)[:,:,::-1])
    img = cv2.resize(img,(224,224)) 

    return torch.tensor(img/255).permute(2,0,1).to('cuda').float(), torch.tensor([target]).float().to('cuda')

"""# 5. Inspect a random image"""

data = Cat_Dog(train_data)

im, label = data[294]

plt.imshow(im.permute(1,2,0).cpu())
print(label)

"""# 6. Define a model, loss funation, and optimizer"""

def conv_layer(ni, no,kernal_size, stride=1):
  return nn.Sequential(
      nn.Conv2d(ni,no, kernal_size, stride),
      nn.ReLU(),
      nn.BatchNorm2d(no),
      nn.MaxPool2d(2)
  )

def build_model():
  cnn_model = nn.Sequential(
      conv_layer(3,64,3),
      conv_layer(64,512,3),
      conv_layer(512,512,3),
      conv_layer(512,512,3),
      conv_layer(512,512,3),
      conv_layer(512,512,3),
      nn.Flatten(),
      nn.Linear(512,1),
      nn.Sigmoid(),
  ).to('cuda')

  loss_function= nn.BCELoss()
  optimizer = torch.optim.Adam(cnn_model.parameters(),lr =1e-3)

  return cnn_model, loss_function, optimizer

cnn_model ,loss_function, optimizer = build_model()

from torchsummary import summary

summary(cnn_model,(3,224,224))

"""# 7. Building a function to fetch the data for the training and validation folders"""

def get_data():
  trainset = Cat_Dog(train_data)
  trainset_dl = DataLoader(trainset, batch_size = 32 , shuffle = True, 
                           drop_last = True)
  valset = Cat_Dog(test_data)
  valset_dl = DataLoader(valset, batch_size = 32,shuffle = True,
                         drop_last = True)
  
  return trainset_dl , valset_dl

"""# 8. Build a function to train the data"""

def train_batch(x,y,model, loss_function,optimizer):
  model.train()
  prediction = model(x)
  batch_loss = loss_function(prediction, y)
  batch_loss.backward()
  optimizer.step()
  optimizer.zero_grad()

  return batch_loss.item()

"""# 9. Build functions to calculate the accuracy and the validation loss


"""

@torch.no_grad()
def accuracy(x,y,model):
  model.eval()
  prediction = model(x)
  is_correct = (prediction>0.5) == y

  return is_correct.cpu().numpy().tolist()

@torch.no_grad()
def valid_loss(x,y,model):
  model.eval()
  prediction = model(x)
  val_loss = loss_function(prediction,y)

  return val_loss.item()

"""# 10. Train the model"""

trainset_dl , valset_dl = get_data()
cnn_model, loss_fn, optimizer = build_model()

train_losses, train_accurcies = [],[]
val_losses , val_accuracies = [], []

for epoch in range(10):
  print(f'Epoch {epoch+1}:')

  epoch_train_losses, epoch_train_accuracies, val_epoch_accuracies = [],[],[]

  for _,batch in enumerate(iter(trainset_dl)):
    x,y = batch
    batch_loss = train_batch(x,y,cnn_model,loss_fn, optimizer)
    epoch_train_losses.append(batch_loss)

  epoch_train_loss = np.array(epoch_train_losses).mean()
  print(f'Train Loss: {epoch_train_loss:0.3f}')
  train_losses.append(epoch_train_loss)

  for _,batch in enumerate(iter(trainset_dl)):
    x,y = batch
    is_correct = accuracy(x,y,cnn_model)
    epoch_train_accuracies.extend(is_correct)
  
  epoch_train_accuracy = np.mean(epoch_train_accuracies)
  print(f'Train Accuracy: {epoch_train_accuracy*100:0.0f}%')
  train_accurcies.append(epoch_train_accuracy)

  for _,batch in enumerate(iter(valset_dl)):
    x,y = batch
    #val_loss = valid_loss(x,y,cnn_model)
    val_acc = accuracy(x,y,cnn_model)
    val_epoch_accuracies.extend(val_acc)
  
  #print(f'Validation Loss: {val_loss:0.3f}')
  #val_losses.append(val_loss)

  val_epoch_accuracy = np.mean(val_epoch_accuracies)
  print(f'Validation Accuracy: {val_epoch_accuracy*100:0.0f}%')
  val_accuracies.append(val_epoch_accuracy)
  
  print('<--------------------------------------------------------->')